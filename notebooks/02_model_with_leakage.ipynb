{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d1a6a0-d437-4afe-a470-cd556763cad1",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:32px; font-family:Georgia; font-weight:bold;\">\n",
    "  Phase 2 - Baseline Model (Leaky Setup)\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "This notebook demonstrates a deliberately flawed modeling pipeline to highlight the impact of <b>patient-level data leakage</b> in medical imaging tasks. A naïve image-level split is applied, and model performance is evaluated under conditions known to produce misleading results. The goal is to document how leakage inflates metrics and to motivate stricter data handling in future phases.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Objectives\n",
    "</h3>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li>Apply a naïve/random train–test split at the image level</li>\n",
    "  <li>Build a baseline convolutional neural network (CNN)</li>\n",
    "  <li>Use raw tensors for image and label access</li>\n",
    "  <li>Train the model on a dataset with known leakage</li>\n",
    "  <li>Evaluate model performance on the test set</li>\n",
    "  <li>Log key metrics: <b>Accuracy</b>, <b>Loss</b>, <b>ROC-AUC</b> (multi-class)</li>\n",
    "  <li>Save model weights and evaluation results</li>\n",
    "  <li>Document why this setup is leaky and why results appear overly optimistic</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Execution Summary\n",
    "</h3>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li>Dataset loaded from Deep Lake with <code>images</code>, <code>labels</code>, and <code>person_num</code> tensors</li>\n",
    "  <li>Label distribution visualized and confirmed across three classes</li>\n",
    "  <li>Sample chest X-rays displayed for Normal and Pneumonia classes</li>\n",
    "  <li>Patient ID distribution analyzed; 61% of patients had multiple images</li>\n",
    "  <li>Image-level split performed using <code>train_test_split</code> with stratification</li>\n",
    "  <li>Leakage verified: <b>435 patients</b> appeared in both train and test sets</li>\n",
    "  <li>Custom PyTorch dataset class implemented with resizing and channel normalization</li>\n",
    "  <li>Simple CNN defined and trained for 5 epochs</li>\n",
    "  <li>Training loss logged per batch and averaged per epoch</li>\n",
    "  <li>Final evaluation yielded:\n",
    "    <ul>\n",
    "      <li><b>Test Accuracy:</b> 0.784</li>\n",
    "      <li><b>Test ROC-AUC (macro, ovr):</b> 0.913</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "<hr>\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Conclusion\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "This phase intentionally demonstrates how image-level splits can leak patient-specific information across partitions. The resulting performance metrics are artificially inflated and do not reflect true generalization. These findings reinforce the need for patient-level separation and motivate the corrected pipeline introduced in Phase 3.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04b24052-9bb7-4171-9ac0-6336dc8b7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9777bd3-3251-4646-aa78-c2d585ba0b0d",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:28px; font-family:Georgia; font-weight:bold;\">\n",
    "Notebook Initialization and Dataset Loading\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "Essential libraries were imported to support data loading, preprocessing, modeling, and evaluation. Random seeds were set for reproducibility across NumPy and PyTorch operations.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li><b>Deep Lake</b> used for dataset access</li>\n",
    "  <li><b>NumPy</b> and <b>Matplotlib</b> for numerical analysis and visualization</li>\n",
    "  <li><b>PyTorch</b> for model construction and training</li>\n",
    "  <li><b>scikit-learn</b> for data splitting and metric evaluation</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "The Chest X‑Ray dataset was loaded from ActiveLoop in read‑only mode. Available tensors include:\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li><code>images</code> — raw chest X‑ray data</li>\n",
    "  <li><code>labels</code> — diagnostic class annotations</li>\n",
    "  <li><code>person_num</code> — patient identifiers</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "Initial inspection confirmed successful dataset access and verified the presence of all required components for downstream analysis.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "093c588a-8cf2-490f-ad21-a8064fc22dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18efdaf9cf0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cee8609d-a84b-43c2-99e3-a53dd6aa7595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/chest-xray-train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/chest-xray-train loaded successfully.\n",
      "\n",
      "Dataset(path='hub://activeloop/chest-xray-train', read_only=True, tensors=['images', 'labels', 'person_num'])\n",
      "Tensors: dict_keys(['images', 'labels', 'person_num'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ds = deeplake.load(\"hub://activeloop/chest-xray-train\")\n",
    "\n",
    "print(ds)\n",
    "print(\"Tensors:\", ds.tensors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa86adc7-52d2-4125-9017-a6ffe3f1147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ds.images\n",
    "labels = ds.labels\n",
    "person_ids = ds.person_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc46dccc-ba68-4063-8602-6ccff98ed62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5216\n",
      "Unique labels: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "labels_list = [int(l[0]) for l in labels.numpy(aslist=True)]\n",
    "\n",
    "print(\"Total samples:\", len(labels_list))\n",
    "print(\"Unique labels:\", set(labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dda1c0-938e-4ca5-a522-0ad07e318773",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:28px; font-family:Georgia; font-weight:bold;\">\n",
    "Image-Level Split and Patient ID Filtering\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "A stratified image-level split was performed using <code>train_test_split</code>, resulting in <b>4172 training samples</b> and <b>1044 test samples</b>. This split was based solely on image labels and did not account for patient identity.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "Patient identifiers were extracted from the dataset to assess split integrity. Out of <b>5216 total samples</b>, <b>1341 entries</b> were missing patient IDs, leaving <b>3875 valid samples</b> for leakage analysis.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li><b>Missing ID ratio:</b> 25.71%</</li>\n",
    "  <li><b>Valid samples used for leakage analysis:</b> 3875</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "This filtering step ensures that only samples with known patient identity are used to verify leakage across train and test partitions.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0999a147-5d9a-4ac3-a8a4-86b8ebd11fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4172\n",
      "Test size: 1044\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(labels_list))\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_list\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6427de12-b0b7-4541-a792-f7817cab2df3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5216\n",
      "Missing patient IDs: 1341\n",
      "Samples with valid patient IDs: 3875\n"
     ]
    }
   ],
   "source": [
    "person_ids_raw = ds.person_num.numpy(aslist=True)\n",
    "\n",
    "clean_person_ids = []\n",
    "missing_count = 0\n",
    "\n",
    "for pid in person_ids_raw:\n",
    "    if pid is None or len(pid) == 0:\n",
    "        clean_person_ids.append(None)\n",
    "        missing_count += 1\n",
    "    else:\n",
    "        clean_person_ids.append(int(pid[0]))\n",
    "\n",
    "print(\"Total samples:\", len(clean_person_ids))\n",
    "print(\"Missing patient IDs:\", missing_count)\n",
    "print(\"Samples with valid patient IDs:\", len(clean_person_ids) - missing_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b97b32bb-a5a1-4e07-92be-6be6e39efc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ID ratio: 25.71%\n"
     ]
    }
   ],
   "source": [
    "missing_ratio = missing_count / len(clean_person_ids)\n",
    "print(f\"Missing ID ratio: {missing_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9faa117e-7b28-4ccc-9cf0-6fbcb30fad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples used for leakage analysis: 3875\n"
     ]
    }
   ],
   "source": [
    "valid_indices = [\n",
    "    i for i, pid in enumerate(clean_person_ids)\n",
    "    if pid is not None\n",
    "]\n",
    "\n",
    "valid_person_ids = [clean_person_ids[i] for i in valid_indices]\n",
    "\n",
    "print(\"Valid samples used for leakage analysis:\", len(valid_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5548dbf-817d-4567-a78f-01c73dbdb353",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:28px; font-family:Georgia; font-weight:bold;\">\n",
    "Patient Leakage Confirmation\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "Patient identifiers were extracted from both training and test sets using image-level indices. The intersection of these sets was computed to identify patients appearing in both partitions.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li><b>Overlapping patients:</b> 435</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "This overlap confirms the presence of <b>patient-level leakage</b>, where multiple images from the same individual are distributed across training and evaluation sets. Such leakage compromises the validity of performance metrics and inflates model accuracy by exposing patient-specific features during training.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81264640-0f4b-4a58-b2a3-9cda28d2c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping patients: 435\n"
     ]
    }
   ],
   "source": [
    "train_patients = set(clean_person_ids[i] for i in train_idx)\n",
    "test_patients = set(clean_person_ids[i] for i in test_idx)\n",
    "\n",
    "overlap = train_patients.intersection(test_patients)\n",
    "\n",
    "print(\"Overlapping patients:\", len(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f3378-abde-4c9b-8136-c63d627c5bc5",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:28px; font-family:Georgia; font-weight:bold;\">\n",
    "Model Training and Runtime Notes\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "The baseline convolutional neural network (CNN) was trained for <b>5 epochs</b> using a batch size of 8 and a dataset of <b>4172 training samples</b>. Training was performed on a leaky image-level split, and progress was logged every 20 batches to monitor convergence.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Training Duration\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "⚠️ <b>Important:</b> This training loop took approximately <b>3–4 hours</b> to complete on standard hardware. Users running this notebook should expect a similar runtime unless using accelerated infrastructure (e.g., GPU clusters).\n",
    "</p>\n",
    "\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Epoch-Level Summary\n",
    "</h3>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li><b>Epoch 1 — Mean Loss:</b> 0.6174</li>\n",
    "  <li><b>Epoch 2 — Mean Loss:</b> 0.4471</li>\n",
    "  <li><b>Epoch 3 — Mean Loss:</b> 0.4059</li>\n",
    "  <li><b>Epoch 4 — Mean Loss:</b> 0.3597</li>\n",
    "  <li><b>Epoch 5 — Mean Loss:</b> 0.3222</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "Loss values decreased steadily across epochs, indicating successful optimization. However, due to patient-level leakage in the data split, these results are not reliable indicators of generalization performance.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Observations\n",
    "</h3>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li>Loss values varied significantly across batches, with occasional spikes and dips</li>\n",
    "  <li>Some batches showed extremely low loss (<code>&lt; 0.1</code>), suggesting memorization of patient-specific features</li>\n",
    "  <li>Later epochs showed tighter clustering of loss values, but leakage undermines interpretability</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "These results reinforce the importance of patient-level partitioning and motivate the corrected pipeline introduced in Phase 3.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94b5018d-ff7b-47e5-bfb7-455ccb1e9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, indices, transform=None):\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        image = images[i].numpy()\n",
    "        label = int(labels_list[i])\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        #  FORCE CHANNEL CONSISTENCY \n",
    "        if image.ndim == 2:\n",
    "            image = image.unsqueeze(0)          # (1, H, W)\n",
    "        elif image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = image.permute(2, 0, 1)      # (3, H, W)\n",
    "            image = image.mean(dim=0, keepdim=True)  \n",
    "        elif image.ndim == 3:\n",
    "            image = image.permute(2, 0, 1)      # (1, H, W)\n",
    "        image = image / 255.0\n",
    "        #  RESIZE \n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image.unsqueeze(0),     # (1, 1, H, W)\n",
    "            size=(224, 224),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze(0)                # (1, 224, 224)\n",
    "    \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b8f5c0a-f785-4d9e-ade8-b906102eedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ChestXrayDataset(train_idx)\n",
    "test_ds  = ChestXrayDataset(test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbbedfef-f99b-48dd-8d47-06983c0cf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 56 * 56, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "308066e1-47c8-4fa8-891e-40b1bbbaa484",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e5e59e3-3b68-41f1-8f50-7276fb18b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Batch 0/522 | Loss 1.0984\n",
      "Epoch 1/5 | Batch 20/522 | Loss 1.1894\n",
      "Epoch 1/5 | Batch 40/522 | Loss 0.9637\n",
      "Epoch 1/5 | Batch 60/522 | Loss 0.7732\n",
      "Epoch 1/5 | Batch 80/522 | Loss 0.5571\n",
      "Epoch 1/5 | Batch 100/522 | Loss 0.7887\n",
      "Epoch 1/5 | Batch 120/522 | Loss 0.6915\n",
      "Epoch 1/5 | Batch 140/522 | Loss 0.9749\n",
      "Epoch 1/5 | Batch 160/522 | Loss 0.4621\n",
      "Epoch 1/5 | Batch 180/522 | Loss 0.5920\n",
      "Epoch 1/5 | Batch 200/522 | Loss 0.7517\n",
      "Epoch 1/5 | Batch 220/522 | Loss 0.4904\n",
      "Epoch 1/5 | Batch 240/522 | Loss 0.3602\n",
      "Epoch 1/5 | Batch 260/522 | Loss 0.5826\n",
      "Epoch 1/5 | Batch 280/522 | Loss 0.3903\n",
      "Epoch 1/5 | Batch 300/522 | Loss 0.7965\n",
      "Epoch 1/5 | Batch 320/522 | Loss 0.5576\n",
      "Epoch 1/5 | Batch 340/522 | Loss 0.4705\n",
      "Epoch 1/5 | Batch 360/522 | Loss 0.2406\n",
      "Epoch 1/5 | Batch 380/522 | Loss 0.4456\n",
      "Epoch 1/5 | Batch 400/522 | Loss 0.5609\n",
      "Epoch 1/5 | Batch 420/522 | Loss 0.3928\n",
      "Epoch 1/5 | Batch 440/522 | Loss 0.3898\n",
      "Epoch 1/5 | Batch 460/522 | Loss 0.5231\n",
      "Epoch 1/5 | Batch 480/522 | Loss 0.6893\n",
      "Epoch 1/5 | Batch 500/522 | Loss 0.6871\n",
      "Epoch 1/5 | Batch 520/522 | Loss 0.3348\n",
      "Epoch 1/5 - Mean Loss: 0.6365\n",
      "Epoch 2/5 | Batch 0/522 | Loss 0.4089\n",
      "Epoch 2/5 | Batch 20/522 | Loss 0.2560\n",
      "Epoch 2/5 | Batch 40/522 | Loss 0.5048\n",
      "Epoch 2/5 | Batch 60/522 | Loss 0.2722\n",
      "Epoch 2/5 | Batch 80/522 | Loss 0.4001\n",
      "Epoch 2/5 | Batch 100/522 | Loss 0.2740\n",
      "Epoch 2/5 | Batch 120/522 | Loss 0.0796\n",
      "Epoch 2/5 | Batch 140/522 | Loss 0.4148\n",
      "Epoch 2/5 | Batch 160/522 | Loss 1.1661\n",
      "Epoch 2/5 | Batch 180/522 | Loss 0.6946\n",
      "Epoch 2/5 | Batch 200/522 | Loss 0.2137\n",
      "Epoch 2/5 | Batch 220/522 | Loss 0.2654\n",
      "Epoch 2/5 | Batch 240/522 | Loss 0.2598\n",
      "Epoch 2/5 | Batch 260/522 | Loss 0.1439\n",
      "Epoch 2/5 | Batch 280/522 | Loss 0.2332\n",
      "Epoch 2/5 | Batch 300/522 | Loss 0.8065\n",
      "Epoch 2/5 | Batch 320/522 | Loss 0.3327\n",
      "Epoch 2/5 | Batch 340/522 | Loss 0.2862\n",
      "Epoch 2/5 | Batch 360/522 | Loss 0.6694\n",
      "Epoch 2/5 | Batch 380/522 | Loss 0.3948\n",
      "Epoch 2/5 | Batch 400/522 | Loss 0.5641\n",
      "Epoch 2/5 | Batch 420/522 | Loss 0.4925\n",
      "Epoch 2/5 | Batch 440/522 | Loss 0.3912\n",
      "Epoch 2/5 | Batch 460/522 | Loss 0.5339\n",
      "Epoch 2/5 | Batch 480/522 | Loss 0.9633\n",
      "Epoch 2/5 | Batch 500/522 | Loss 0.1184\n",
      "Epoch 2/5 | Batch 520/522 | Loss 0.8282\n",
      "Epoch 2/5 - Mean Loss: 0.4590\n",
      "Epoch 3/5 | Batch 0/522 | Loss 0.3149\n",
      "Epoch 3/5 | Batch 20/522 | Loss 0.3977\n",
      "Epoch 3/5 | Batch 40/522 | Loss 0.5467\n",
      "Epoch 3/5 | Batch 60/522 | Loss 0.2029\n",
      "Epoch 3/5 | Batch 80/522 | Loss 0.2077\n",
      "Epoch 3/5 | Batch 100/522 | Loss 0.2815\n",
      "Epoch 3/5 | Batch 120/522 | Loss 0.3395\n",
      "Epoch 3/5 | Batch 140/522 | Loss 0.2367\n",
      "Epoch 3/5 | Batch 160/522 | Loss 0.4746\n",
      "Epoch 3/5 | Batch 180/522 | Loss 0.2313\n",
      "Epoch 3/5 | Batch 200/522 | Loss 0.4346\n",
      "Epoch 3/5 | Batch 220/522 | Loss 0.4991\n",
      "Epoch 3/5 | Batch 240/522 | Loss 0.5257\n",
      "Epoch 3/5 | Batch 260/522 | Loss 0.0915\n",
      "Epoch 3/5 | Batch 280/522 | Loss 0.3704\n",
      "Epoch 3/5 | Batch 300/522 | Loss 0.1281\n",
      "Epoch 3/5 | Batch 320/522 | Loss 0.1581\n",
      "Epoch 3/5 | Batch 340/522 | Loss 0.7447\n",
      "Epoch 3/5 | Batch 360/522 | Loss 0.3407\n",
      "Epoch 3/5 | Batch 380/522 | Loss 1.0813\n",
      "Epoch 3/5 | Batch 400/522 | Loss 0.1982\n",
      "Epoch 3/5 | Batch 420/522 | Loss 0.9723\n",
      "Epoch 3/5 | Batch 440/522 | Loss 0.3703\n",
      "Epoch 3/5 | Batch 460/522 | Loss 0.4620\n",
      "Epoch 3/5 | Batch 480/522 | Loss 0.5204\n",
      "Epoch 3/5 | Batch 500/522 | Loss 0.2565\n",
      "Epoch 3/5 | Batch 520/522 | Loss 0.3529\n",
      "Epoch 3/5 - Mean Loss: 0.4262\n",
      "Epoch 4/5 | Batch 0/522 | Loss 0.6113\n",
      "Epoch 4/5 | Batch 20/522 | Loss 0.0906\n",
      "Epoch 4/5 | Batch 40/522 | Loss 0.3985\n",
      "Epoch 4/5 | Batch 60/522 | Loss 0.2558\n",
      "Epoch 4/5 | Batch 80/522 | Loss 0.3130\n",
      "Epoch 4/5 | Batch 100/522 | Loss 0.3573\n",
      "Epoch 4/5 | Batch 120/522 | Loss 0.8610\n",
      "Epoch 4/5 | Batch 140/522 | Loss 0.8215\n",
      "Epoch 4/5 | Batch 160/522 | Loss 0.2406\n",
      "Epoch 4/5 | Batch 180/522 | Loss 0.5137\n",
      "Epoch 4/5 | Batch 200/522 | Loss 0.5967\n",
      "Epoch 4/5 | Batch 220/522 | Loss 0.3008\n",
      "Epoch 4/5 | Batch 240/522 | Loss 0.2576\n",
      "Epoch 4/5 | Batch 260/522 | Loss 0.2936\n",
      "Epoch 4/5 | Batch 280/522 | Loss 0.7549\n",
      "Epoch 4/5 | Batch 300/522 | Loss 0.4657\n",
      "Epoch 4/5 | Batch 320/522 | Loss 0.2707\n",
      "Epoch 4/5 | Batch 340/522 | Loss 0.6492\n",
      "Epoch 4/5 | Batch 360/522 | Loss 0.6751\n",
      "Epoch 4/5 | Batch 380/522 | Loss 0.8459\n",
      "Epoch 4/5 | Batch 400/522 | Loss 0.5124\n",
      "Epoch 4/5 | Batch 420/522 | Loss 0.3553\n",
      "Epoch 4/5 | Batch 440/522 | Loss 0.6125\n",
      "Epoch 4/5 | Batch 460/522 | Loss 0.3932\n",
      "Epoch 4/5 | Batch 480/522 | Loss 0.1130\n",
      "Epoch 4/5 | Batch 500/522 | Loss 0.1269\n",
      "Epoch 4/5 | Batch 520/522 | Loss 0.3361\n",
      "Epoch 4/5 - Mean Loss: 0.4045\n",
      "Epoch 5/5 | Batch 0/522 | Loss 0.3452\n",
      "Epoch 5/5 | Batch 20/522 | Loss 0.3932\n",
      "Epoch 5/5 | Batch 40/522 | Loss 0.1811\n",
      "Epoch 5/5 | Batch 60/522 | Loss 0.1712\n",
      "Epoch 5/5 | Batch 80/522 | Loss 0.0539\n",
      "Epoch 5/5 | Batch 100/522 | Loss 0.1779\n",
      "Epoch 5/5 | Batch 120/522 | Loss 0.3625\n",
      "Epoch 5/5 | Batch 140/522 | Loss 0.4452\n",
      "Epoch 5/5 | Batch 160/522 | Loss 0.2593\n",
      "Epoch 5/5 | Batch 180/522 | Loss 0.2254\n",
      "Epoch 5/5 | Batch 200/522 | Loss 0.5087\n",
      "Epoch 5/5 | Batch 220/522 | Loss 0.4013\n",
      "Epoch 5/5 | Batch 240/522 | Loss 0.4937\n",
      "Epoch 5/5 | Batch 260/522 | Loss 0.2915\n",
      "Epoch 5/5 | Batch 280/522 | Loss 0.2033\n",
      "Epoch 5/5 | Batch 300/522 | Loss 0.0360\n",
      "Epoch 5/5 | Batch 320/522 | Loss 0.3678\n",
      "Epoch 5/5 | Batch 340/522 | Loss 0.4776\n",
      "Epoch 5/5 | Batch 360/522 | Loss 0.2954\n",
      "Epoch 5/5 | Batch 380/522 | Loss 0.5083\n",
      "Epoch 5/5 | Batch 400/522 | Loss 0.2651\n",
      "Epoch 5/5 | Batch 420/522 | Loss 0.3688\n",
      "Epoch 5/5 | Batch 440/522 | Loss 0.3654\n",
      "Epoch 5/5 | Batch 460/522 | Loss 0.8938\n",
      "Epoch 5/5 | Batch 480/522 | Loss 0.5223\n",
      "Epoch 5/5 | Batch 500/522 | Loss 0.2344\n",
      "Epoch 5/5 | Batch 520/522 | Loss 0.6689\n",
      "Epoch 5/5 - Mean Loss: 0.3792\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                f\"Batch {batch_idx}/{len(train_loader)} | \"\n",
    "                f\"Loss {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Mean Loss: {np.mean(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d2123-bee9-4665-97c6-82debc5dd078",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:22px; font-family:Georgia; font-weight:bold;\">\n",
    " Leaky CNN - Phase 2 Evaluation\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "This convolutional neural network (CNN) was trained under a flawed setup where patient-level leakage was present. The train–test split was performed at the image level, resulting in <b>435 patients</b> appearing in both training and test sets. This allowed the model to learn patient-specific features, artificially boosting performance.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Evaluation Metrics\n",
    "</h3>\n",
    "\n",
    "<ul style=\"font-size:15px; font-family:Arial;\">\n",
    "  <li><b>Test Accuracy:</b> 0.784</li>\n",
    "  <li><b>Test ROC-AUC (macro, ovr):</b> 0.913</li>\n",
    "  <li><b>Test F1-Score (macro):</b> 0.744</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"font-size:20px; font-family:Georgia; font-weight:bold;\">\n",
    "Interpretation\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "While the metrics appear strong, they are misleading. The presence of patient-level leakage means the model likely memorized features from individuals it saw during training, which reappeared in the test set. As a result, these scores do not reflect true generalization and should not be used to assess clinical reliability.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; font-family:Arial;\">\n",
    "This evaluation serves as a cautionary baseline and reinforces the importance of group-aware splitting and leak-free pipelines in medical machine learning workflows.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b457052-cdf5-4a58-91f9-2635002cca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.784\n",
      "Test ROC-AUC (macro, ovr): 0.913\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)                         \n",
    "        probs = torch.softmax(outputs, dim=1)      \n",
    "\n",
    "        y_true.extend(y.numpy())                   \n",
    "        y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        y_prob.extend(probs.cpu().numpy())         \n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_prob = np.array(y_prob)                          \n",
    "\n",
    "auc = roc_auc_score(\n",
    "    y_true,\n",
    "    y_prob,\n",
    "    multi_class=\"ovr\",      \n",
    "    average=\"macro\"         \n",
    ")\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "print(f\"Test ROC-AUC (macro, ovr): {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8540540-0196-4eac-9d4e-a4d681f96744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-Score: 0.744\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Test F1-Score: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
